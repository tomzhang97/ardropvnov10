CMD: /home/tom/.conda/envs/ardrop/bin/python experiments/hotpot_dev_predict_distractor.py --dev_json runs/_shards/dev_4000_5000.json --out_pred runs/_shards/pred_4000_5000.json --device 0 --evidence_k 8 --sp_k 2
======================================================================
HOTPOT QA PREDICTION
======================================================================
LLM Model: meta-llama/Meta-Llama-3-8B-Instruct
Embed Model: sentence-transformers/all-MiniLM-L6-v2
Evidence K: 8, SP K: 2
Device: GPU 0
Answer Type Detection: True
Answer-Guided SP: True
======================================================================

Loading dev set: runs/_shards/dev_4000_5000.json
Loading embedder: sentence-transformers/all-MiniLM-L6-v2
Loading LLM: meta-llama/Meta-Llama-3-8B-Instruct
[CHECKPOINT 0/5] Calling get_llm for model: meta-llama/Meta-Llama-3-8B-Instruct
[CHECKPOINT 1/5] Initializing LocalLLM with model: meta-llama/Meta-Llama-3-8B-Instruct
[CHECKPOINT 2/5] Loading tokenizer...
[CHECKPOINT 3/5] Tokenizer loaded.
[CHECKPOINT 4/5] Loading model...
[CHECKPOINT 4.1/5] Using CUDA device: 0

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  2.15it/s]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:00<00:00,  2.35it/s]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:01<00:00,  2.47it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:01<00:00,  2.98it/s]




======================================================================
PREDICTION COMPLETE
======================================================================
Predictions: runs/_shards/pred_4000_5000.json
Metrics: runs/_shards/pred_4000_5000_metrics.json

ðŸ“Š PERFORMANCE:
  Examples: 1000
  Avg Latency: 684.54ms
  Avg Tokens: 425.31
  Throughput: 1.46 ex/sec
======================================================================